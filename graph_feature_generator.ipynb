{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Feature Generator\n",
    "---\n",
    "**Purpose:** Add graph-based AML features to the transaction table (with additional features).\n",
    "\n",
    "**Input:** `transaction_additional_feature.parquet` (or CSV fallback) from `outputs/`.\n",
    "\n",
    "**Output:** `transaction_with_graph_features.parquet` in `outputs/`.\n",
    "\n",
    "**Features:**\n",
    "- **Sender-side:** in/out degree, total inflow/outflow, unique counterparties, repeat counterparty ratio (30d).\n",
    "- **Receiver-side:** in/out degree, unique senders (30d) — for internal transfers.\n",
    "- **High-signal:** pass-through ratios (24h/7d), outflow-to-inflow ratio, avg time gap in→out, accounts/devices per device/account, device shared high-risk ratio, shared device fraud count.\n",
    "\n",
    "Config is in `config/base_config.ipynb` under `GRAPH_FEATURE_CONFIG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n",
      "  Input:  outputs/transaction_additional_feature.parquet\n",
      "  Output: outputs/transaction_with_graph_features.parquet\n",
      "  Rolling: 30d / 7d / 24h\n"
     ]
    }
   ],
   "source": [
    "# ── Imports & config ─────────────────────────────────────────────────────────\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Config: set GRAPH_FEATURE_CONFIG in config/base_config.ipynb (section 11b) or use inline below.\n",
    "try:\n",
    "    from config.base_config import GRAPH_FEATURE_CONFIG\n",
    "except Exception:\n",
    "    GRAPH_FEATURE_CONFIG = {\n",
    "        \"input_path\"       : \"transaction_additional_feature.parquet\",\n",
    "        \"output_path\"      : \"transaction_with_graph_features.parquet\",\n",
    "        \"input_dir\"        : \"outputs\",\n",
    "        \"rolling_days\"     : 30,\n",
    "        \"rolling_days_7d\"  : 7,\n",
    "        \"rolling_hours_24h\": 24,\n",
    "        \"features_enabled\" : [\n",
    "            \"sender_in_degree_30d\", \"sender_out_degree_30d\", \"sender_total_inflow_30d\",\n",
    "            \"sender_total_outflow_30d\", \"sender_unique_counterparties_30d\", \"sender_repeat_counterparty_ratio\",\n",
    "            \"receiver_in_degree_30d\", \"receiver_out_degree_30d\", \"receiver_unique_senders_30d\",\n",
    "            \"pass_through_ratio_24h\", \"pass_through_ratio_7d\", \"outflow_to_inflow_ratio_7d\",\n",
    "            \"avg_time_gap_in_out\", \"accounts_per_device\", \"devices_per_account\",\n",
    "            \"device_shared_high_risk_ratio\", \"shared_device_fraud_count\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "INPUT_DIR   = Path(GRAPH_FEATURE_CONFIG[\"input_dir\"])\n",
    "INPUT_PATH  = INPUT_DIR / GRAPH_FEATURE_CONFIG[\"input_path\"]\n",
    "OUTPUT_PATH = INPUT_DIR / GRAPH_FEATURE_CONFIG[\"output_path\"]\n",
    "ROLLING_30D = f\"{GRAPH_FEATURE_CONFIG['rolling_days']}d\"\n",
    "ROLLING_7D  = f\"{GRAPH_FEATURE_CONFIG['rolling_days_7d']}d\"\n",
    "ROLLING_24H = f\"{GRAPH_FEATURE_CONFIG['rolling_hours_24h']}h\"\n",
    "FEATURES    = GRAPH_FEATURE_CONFIG[\"features_enabled\"]\n",
    "\n",
    "def load_transactions() -> pd.DataFrame:\n",
    "    if INPUT_PATH.suffix == \".parquet\" and INPUT_PATH.exists():\n",
    "        return pd.read_parquet(INPUT_PATH)\n",
    "    csv_path = INPUT_PATH.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        return df\n",
    "    alt = INPUT_DIR / \"transactions_additional_features.csv\"\n",
    "    if alt.exists():\n",
    "        df = pd.read_csv(alt)\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        return df\n",
    "    fallback = INPUT_DIR / \"transactions.csv\"\n",
    "    if fallback.exists():\n",
    "        df = pd.read_csv(fallback)\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        return df\n",
    "    raise FileNotFoundError(f\"No input found. Tried: {INPUT_PATH}, {csv_path}, {alt}, {fallback}\")\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(f\"  Input:  {INPUT_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_PATH}\")\n",
    "print(f\"  Rolling: 30d / 7d / 24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 86,992 rows, 109 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>sender_account_id</th>\n",
       "      <th>receiver_account_id</th>\n",
       "      <th>beneficiary_id</th>\n",
       "      <th>device_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>channel</th>\n",
       "      <th>debit_credit</th>\n",
       "      <th>...</th>\n",
       "      <th>rule_very_high_risk_offshore</th>\n",
       "      <th>rule_low_kyc_offshore</th>\n",
       "      <th>rule_low_income_large_txn</th>\n",
       "      <th>rule_vpn_offshore</th>\n",
       "      <th>rule_emulator_crypto</th>\n",
       "      <th>rule_new_account_offshore</th>\n",
       "      <th>rule_new_acct_high_cust_velocity</th>\n",
       "      <th>rule_trigger_count</th>\n",
       "      <th>max_rule_severity</th>\n",
       "      <th>weighted_rule_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 00:04:29</td>\n",
       "      <td>T60660</td>\n",
       "      <td>C45</td>\n",
       "      <td>A1209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B244</td>\n",
       "      <td>D134</td>\n",
       "      <td>15438.75</td>\n",
       "      <td>web</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01 00:04:35</td>\n",
       "      <td>T6942</td>\n",
       "      <td>C395</td>\n",
       "      <td>A139</td>\n",
       "      <td>A1051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D597</td>\n",
       "      <td>17698.42</td>\n",
       "      <td>web</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp transaction_id customer_id sender_account_id  \\\n",
       "0 2025-09-01 00:04:29         T60660         C45             A1209   \n",
       "1 2025-09-01 00:04:35          T6942        C395              A139   \n",
       "\n",
       "  receiver_account_id beneficiary_id device_id    amount channel debit_credit  \\\n",
       "0                 NaN           B244      D134  15438.75     web        debit   \n",
       "1               A1051            NaN      D597  17698.42     web        debit   \n",
       "\n",
       "   ... rule_very_high_risk_offshore  rule_low_kyc_offshore  \\\n",
       "0  ...                            0                      0   \n",
       "1  ...                            0                      0   \n",
       "\n",
       "  rule_low_income_large_txn  rule_vpn_offshore  rule_emulator_crypto  \\\n",
       "0                         0                  0                     0   \n",
       "1                         0                  0                     0   \n",
       "\n",
       "   rule_new_account_offshore rule_new_acct_high_cust_velocity  \\\n",
       "0                          0                                0   \n",
       "1                          0                                0   \n",
       "\n",
       "  rule_trigger_count max_rule_severity weighted_rule_score  \n",
       "0                  2                 3                   5  \n",
       "1                  1                 2                   2  \n",
       "\n",
       "[2 rows x 109 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Load data ─────────────────────────────────────────────────────────────────\n",
    "df = load_transactions()\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "required = [\"transaction_id\", \"sender_account_id\", \"receiver_account_id\", \"beneficiary_id\", \"device_id\", \"timestamp\", \"amount\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "if \"label\" not in df.columns:\n",
    "    df[\"label\"] = 0\n",
    "if \"high_risk_beneficiary\" not in df.columns:\n",
    "    df[\"high_risk_beneficiary\"] = 0\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows, {df.shape[1]} columns.\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflow events (internal): 32888\n",
      "Outflow events (all): 86992\n"
     ]
    }
   ],
   "source": [
    "# ── Counterparty column (receiver account or beneficiary) ─────────────────────\n",
    "df[\"_counterparty\"] = df[\"receiver_account_id\"].fillna(df[\"beneficiary_id\"].astype(str))\n",
    "\n",
    "# Inflow events: rows where this account is the receiver (internal only)\n",
    "inflow = df.loc[df[\"receiver_account_id\"].notna(), [\"receiver_account_id\", \"sender_account_id\", \"timestamp\", \"amount\"]].copy()\n",
    "inflow = inflow.rename(columns={\"receiver_account_id\": \"account_id\", \"sender_account_id\": \"sender_id\"})\n",
    "inflow = inflow.sort_values([\"account_id\", \"timestamp\"])\n",
    "\n",
    "# Outflow events: all rows (sender, counterparty, device, amount, timestamp)\n",
    "outflow = df[[\"sender_account_id\", \"timestamp\", \"amount\", \"_counterparty\", \"device_id\", \"label\", \"high_risk_beneficiary\"]].copy()\n",
    "outflow = outflow.rename(columns={\"sender_account_id\": \"account_id\"})\n",
    "outflow = outflow.sort_values([\"account_id\", \"timestamp\"])\n",
    "\n",
    "print(\"Inflow events (internal):\", len(inflow))\n",
    "print(\"Outflow events (all):\", len(outflow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'int' and 'Timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'Timestamp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     r\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msender_in_degree_30d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msender_total_inflow_30d\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m---> 42\u001b[0m sender_out_30 \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_sender_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m sender_in_30  \u001b[38;5;241m=\u001b[39m rolling_sender_in()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender outflow rolling:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sender_out_30\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mrolling_sender_out\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m r \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mrolling(ROLLING_30D, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     25\u001b[0m r\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msender_out_degree_30d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msender_total_outflow_30d\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m cp \u001b[38;5;241m=\u001b[39m \u001b[43m_rolling_nunique\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccount_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_counterparty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWINDOW_30D_TD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msender_unique_counterparties_30d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m dev \u001b[38;5;241m=\u001b[39m _rolling_nunique(outflow, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, WINDOW_30D_TD, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevices_per_account\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m r \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mmerge(cp, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmerge(dev, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccount_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36m_rolling_nunique\u001b[0;34m(df, group_col, time_col, value_col, window_td, out_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m t_end \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimestamp(ts[i])\n\u001b[1;32m     11\u001b[0m t_start \u001b[38;5;241m=\u001b[39m t_end \u001b[38;5;241m-\u001b[39m window_td\n\u001b[0;32m---> 12\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m window_vals \u001b[38;5;241m=\u001b[39m vals[start_idx : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m out\u001b[38;5;241m.\u001b[39mappend((g[group_col]\u001b[38;5;241m.\u001b[39miloc[i], t_end, \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(window_vals))))\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:1527\u001b[0m, in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearchsorted\u001b[39m(a, v, side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, sorter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;124;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;124;03m    30  # The element at index 2 of the sorted array is 30.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearchsorted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:46\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m arr, \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mas_arrays(subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv\u001b[38;5;241m.\u001b[39mwrap(result, to_scalar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'int' and 'Timestamp'"
     ]
    }
   ],
   "source": [
    "# ── Sender-side rolling 30d (outflow + inflow for sender) ──────────────────────\n",
    "# Rolling nunique not supported on object cols; compute per-group with time window.\n",
    "def _rolling_nunique(df, group_col, time_col, value_col, window_td, out_name):\n",
    "    out = []\n",
    "    for _, g in df.groupby(group_col, sort=False):\n",
    "        g = g.sort_values(time_col).reset_index(drop=True)\n",
    "        ts = pd.to_datetime(g[time_col]).values\n",
    "        vals = g[value_col].values\n",
    "        for i in range(len(g)):\n",
    "            t_end = ts[i]\n",
    "            t_start = t_end - np.timedelta64(int(window_td.total_seconds() * 1e9), \"ns\")\n",
    "            start_idx = np.searchsorted(ts, t_start, side=\"right\")\n",
    "            window_vals = vals[start_idx : i + 1]\n",
    "            out.append((g[group_col].iloc[i], pd.Timestamp(t_end), len(np.unique(window_vals))))\n",
    "    res = pd.DataFrame(out, columns=[group_col, time_col, out_name])\n",
    "    res[time_col] = pd.to_datetime(res[time_col])\n",
    "    return res\n",
    "\n",
    "WINDOW_30D_TD = pd.Timedelta(ROLLING_30D)\n",
    "\n",
    "def rolling_sender_out():\n",
    "    o = outflow.set_index(\"timestamp\", drop=False)\n",
    "    g = o.groupby(\"account_id\", group_keys=False)\n",
    "    r = g.rolling(ROLLING_30D, on=\"timestamp\").agg({\"amount\": [\"count\", \"sum\"]}).reset_index()\n",
    "    r.columns = [\"account_id\", \"timestamp\", \"sender_out_degree_30d\", \"sender_total_outflow_30d\"]\n",
    "    cp = _rolling_nunique(outflow, \"account_id\", \"timestamp\", \"_counterparty\", WINDOW_30D_TD, \"sender_unique_counterparties_30d\")\n",
    "    dev = _rolling_nunique(outflow, \"account_id\", \"timestamp\", \"device_id\", WINDOW_30D_TD, \"devices_per_account\")\n",
    "    r = r.merge(cp, on=[\"account_id\", \"timestamp\"]).merge(dev, on=[\"account_id\", \"timestamp\"])\n",
    "    r[\"sender_repeat_counterparty_ratio\"] = (\n",
    "        1 - r[\"sender_unique_counterparties_30d\"] / r[\"sender_out_degree_30d\"].replace(0, np.nan)\n",
    "    ).fillna(0).round(4)\n",
    "    return r\n",
    "\n",
    "def rolling_sender_in():\n",
    "    i = inflow.set_index(\"timestamp\", drop=False)\n",
    "    r = i.groupby(\"account_id\", group_keys=False).rolling(ROLLING_30D, on=\"timestamp\").agg(\n",
    "        {\"amount\": [\"count\", \"sum\"]}\n",
    "    ).reset_index()\n",
    "    r.columns = [\"account_id\", \"timestamp\", \"sender_in_degree_30d\", \"sender_total_inflow_30d\"]\n",
    "    return r\n",
    "\n",
    "sender_out_30 = rolling_sender_out()\n",
    "sender_in_30  = rolling_sender_in()\n",
    "print(\"Sender outflow rolling:\", sender_out_30.shape)\n",
    "print(\"Sender inflow rolling:\", sender_in_30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Receiver-side rolling 30d (for internal transfers: receiver = account) ─────\n",
    "def rolling_receiver():\n",
    "    i = inflow.copy()\n",
    "    i = i.rename(columns={\"account_id\": \"receiver_id\", \"sender_id\": \"sender_id\"})\n",
    "    i = i.set_index(\"timestamp\", drop=False)\n",
    "    r = i.groupby(\"receiver_id\", group_keys=False).rolling(ROLLING_30D, on=\"timestamp\").agg({\"amount\": [\"count\", \"sum\"]}).reset_index()\n",
    "    r.columns = [\"account_id\", \"timestamp\", \"receiver_in_degree_30d\", \"receiver_total_inflow_30d\"]\n",
    "    su = _rolling_nunique(i.reset_index(drop=True), \"receiver_id\", \"timestamp\", \"sender_id\", WINDOW_30D_TD, \"receiver_unique_senders_30d\")\n",
    "    su = su.rename(columns={\"receiver_id\": \"account_id\"})\n",
    "    r = r.merge(su, on=[\"account_id\", \"timestamp\"])\n",
    "    return r\n",
    "\n",
    "# receiver_out_degree_30d merged later from sender_out_30 (same account as sender)\n",
    "receiver_in_30 = rolling_receiver()\n",
    "print(\"Receiver inflow rolling:\", receiver_in_30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 24h / 7d inflow & outflow for pass-through and ratio ─────────────────────────\n",
    "def rolling_inflow_24h_7d():\n",
    "    i = inflow.set_index(\"timestamp\", drop=False)\n",
    "    g = i.groupby(\"account_id\", group_keys=False)\n",
    "    r24 = g.rolling(ROLLING_24H, on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r24 = r24.rename(columns={\"amount\": \"sender_total_inflow_24h\"})\n",
    "    r7  = g.rolling(ROLLING_7D,  on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r7  = r7.rename(columns={\"amount\": \"sender_total_inflow_7d\"})\n",
    "    return r24, r7\n",
    "\n",
    "def rolling_outflow_24h_7d():\n",
    "    o = outflow.set_index(\"timestamp\", drop=False)\n",
    "    g = o.groupby(\"account_id\", group_keys=False)\n",
    "    r24 = g.rolling(ROLLING_24H, on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r24 = r24.rename(columns={\"amount\": \"sender_total_outflow_24h\"})\n",
    "    r7  = g.rolling(ROLLING_7D,  on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r7  = r7.rename(columns={\"amount\": \"sender_total_outflow_7d\"})\n",
    "    return r24, r7\n",
    "\n",
    "in_24, in_7  = rolling_inflow_24h_7d()\n",
    "out_24, out_7 = rolling_outflow_24h_7d()\n",
    "print(\"24h/7d inflow and outflow rolling done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Device rolling: accounts_per_device, device_shared_high_risk_ratio, shared_device_fraud_count ──\n",
    "dev = df[[\"device_id\", \"timestamp\", \"sender_account_id\", \"label\", \"high_risk_beneficiary\"]].copy()\n",
    "dev = dev.sort_values([\"device_id\", \"timestamp\"]).set_index(\"timestamp\", drop=False)\n",
    "g = dev.groupby(\"device_id\", group_keys=False)\n",
    "r = g.rolling(ROLLING_30D, on=\"timestamp\").agg({\"sender_account_id\": \"count\", \"label\": \"sum\", \"high_risk_beneficiary\": \"sum\"}).reset_index()\n",
    "r.columns = [\"device_id\", \"timestamp\", \"_dev_txn_count\", \"_dev_fraud_count\", \"_dev_high_risk_count\"]\n",
    "dev_df = dev.reset_index(drop=True)\n",
    "nacc = _rolling_nunique(dev_df, \"device_id\", \"timestamp\", \"sender_account_id\", WINDOW_30D_TD, \"accounts_per_device\")\n",
    "device_rolling = r.merge(nacc, on=[\"device_id\", \"timestamp\"])\n",
    "device_rolling = device_rolling[[\"device_id\", \"timestamp\", \"accounts_per_device\", \"_dev_txn_count\", \"_dev_fraud_count\", \"_dev_high_risk_count\"]]\n",
    "device_rolling[\"device_shared_high_risk_ratio\"] = (\n",
    "    (device_rolling[\"_dev_fraud_count\"] + device_rolling[\"_dev_high_risk_count\"])\n",
    "    / device_rolling[\"_dev_txn_count\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "device_rolling[\"shared_device_fraud_count\"] = device_rolling[\"_dev_fraud_count\"]\n",
    "device_rolling = device_rolling.drop(columns=[\"_dev_txn_count\", \"_dev_fraud_count\", \"_dev_high_risk_count\"])\n",
    "print(\"Device rolling:\", device_rolling.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── avg_time_gap_in_out: per account, average time from inflow to next outflow ──\n",
    "def avg_time_gap_per_account() -> pd.DataFrame:\n",
    "    gaps = []\n",
    "    for acc, grp_in in inflow.groupby(\"account_id\"):\n",
    "        grp_out = outflow[outflow[\"account_id\"] == acc].sort_values(\"timestamp\")\n",
    "        if grp_out.empty:\n",
    "            continue\n",
    "        ts_out = pd.to_datetime(grp_out[\"timestamp\"]).values\n",
    "        for _, row in grp_in.iterrows():\n",
    "            t_in = np.datetime64(pd.Timestamp(row[\"timestamp\"]))\n",
    "            idx = np.searchsorted(ts_out, t_in, side=\"right\")\n",
    "            if idx < len(ts_out):\n",
    "                gap_sec = (ts_out[idx] - t_in) / np.timedelta64(1, \"s\")\n",
    "                gaps.append({\"account_id\": acc, \"timestamp\": t_in, \"_gap_sec\": gap_sec})\n",
    "    if not gaps:\n",
    "        return pd.DataFrame(columns=[\"account_id\", \"timestamp\", \"avg_time_gap_in_out\"])\n",
    "    gap_df = pd.DataFrame(gaps)\n",
    "    gap_df[\"timestamp\"] = pd.to_datetime(gap_df[\"timestamp\"])\n",
    "    avg_by_account = gap_df.groupby(\"account_id\")[\"_gap_sec\"].mean().reset_index()\n",
    "    avg_by_account = avg_by_account.rename(columns={\"_gap_sec\": \"avg_time_gap_in_out\"})\n",
    "    return avg_by_account\n",
    "\n",
    "avg_gap_df = avg_time_gap_per_account()\n",
    "print(\"Avg time gap (in→out) computed for\", len(avg_gap_df), \"accounts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Merge all rolling stats back to main df (by exact timestamp + key) ──────────\n",
    "# We need to merge on (sender_account_id, timestamp) for sender stats; (receiver_account_id, timestamp) for receiver; (device_id, timestamp) for device.\n",
    "# Rolling tables have one row per (key, timestamp) so we merge on (key, timestamp).\n",
    "\n",
    "merge_cols = [\"account_id\", \"timestamp\"]\n",
    "\n",
    "sender_out_cols = [\"sender_out_degree_30d\", \"sender_total_outflow_30d\", \"sender_unique_counterparties_30d\",\n",
    "                   \"sender_repeat_counterparty_ratio\", \"devices_per_account\"]\n",
    "df = df.merge(\n",
    "    sender_out_30[merge_cols + sender_out_cols],\n",
    "    left_on=[\"sender_account_id\", \"timestamp\"],\n",
    "    right_on=merge_cols,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_sout\")\n",
    ").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "sender_in_cols = [\"sender_in_degree_30d\", \"sender_total_inflow_30d\"]\n",
    "df = df.merge(\n",
    "    sender_in_30[merge_cols + sender_in_cols],\n",
    "    left_on=[\"sender_account_id\", \"timestamp\"],\n",
    "    right_on=merge_cols,\n",
    "    how=\"left\"\n",
    ")\n",
    "if \"account_id\" in df.columns and df[\"account_id\"].equals(df[\"sender_account_id\"]):\n",
    "    df = df.drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "in_24_cols = [\"sender_total_inflow_24h\"]\n",
    "in_7_cols  = [\"sender_total_inflow_7d\"]\n",
    "out_24_cols = [\"sender_total_outflow_24h\"]\n",
    "out_7_cols  = [\"sender_total_outflow_7d\"]\n",
    "df = df.merge(in_24[merge_cols + in_24_cols],   left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "df = df.merge(in_7[merge_cols + in_7_cols],     left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "df = df.merge(out_24[merge_cols + out_24_cols], left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "df = df.merge(out_7[merge_cols + out_7_cols],   left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "df[\"pass_through_ratio_24h\"] = (\n",
    "    np.minimum(df[\"sender_total_inflow_24h\"].fillna(0), df[\"sender_total_outflow_24h\"].fillna(0))\n",
    "    / df[\"sender_total_inflow_24h\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "df[\"pass_through_ratio_7d\"] = (\n",
    "    np.minimum(df[\"sender_total_inflow_7d\"].fillna(0), df[\"sender_total_outflow_7d\"].fillna(0))\n",
    "    / df[\"sender_total_inflow_7d\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "df[\"outflow_to_inflow_ratio_7d\"] = (\n",
    "    df[\"sender_total_outflow_7d\"].fillna(0) / df[\"sender_total_inflow_7d\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "\n",
    "df = df.merge(\n",
    "    device_rolling[[\"device_id\", \"timestamp\", \"accounts_per_device\", \"device_shared_high_risk_ratio\", \"shared_device_fraud_count\"]],\n",
    "    on=[\"device_id\", \"timestamp\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df = df.merge(\n",
    "    avg_gap_df,\n",
    "    left_on=\"sender_account_id\",\n",
    "    right_on=\"account_id\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "receiver_in_cols = [\"receiver_in_degree_30d\", \"receiver_total_inflow_30d\", \"receiver_unique_senders_30d\"]\n",
    "receiver_in_30_renamed = receiver_in_30.rename(columns={\"account_id\": \"receiver_account_id\"})\n",
    "df = df.merge(\n",
    "    receiver_in_30_renamed[[\"receiver_account_id\", \"timestamp\"] + receiver_in_cols],\n",
    "    on=[\"receiver_account_id\", \"timestamp\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "receiver_out_30_for_merge = sender_out_30[[\"account_id\", \"timestamp\", \"sender_out_degree_30d\"]].rename(\n",
    "    columns={\"account_id\": \"receiver_account_id\", \"sender_out_degree_30d\": \"receiver_out_degree_30d\"}\n",
    ")\n",
    "df = df.merge(receiver_out_30_for_merge, on=[\"receiver_account_id\", \"timestamp\"], how=\"left\")\n",
    "\n",
    "print(\"Merged. Columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Drop helper columns and keep only requested features if config says so ─────\n",
    "df = df.drop(columns=[\"_counterparty\"], errors=\"ignore\")\n",
    "\n",
    "optional_drop = [\"sender_total_inflow_24h\", \"sender_total_outflow_24h\", \"sender_total_inflow_7d\", \"sender_total_outflow_7d\"]\n",
    "for c in optional_drop:\n",
    "    if c in df.columns and c not in FEATURES:\n",
    "        df = df.drop(columns=[c], errors=\"ignore\")\n",
    "\n",
    "print(\"Final columns:\", len(df.columns))\n",
    "graph_cols = [c for c in df.columns if any(x in c for x in [\"degree\", \"inflow\", \"outflow\", \"counterpart\", \"pass_through\", \"ratio_7d\", \"avg_time_gap\", \"accounts_per_device\", \"devices_per\", \"device_shared\", \"shared_device_fraud\", \"receiver_in\", \"receiver_out\", \"receiver_unique\"])]\n",
    "print(\"Graph feature columns:\", graph_cols[:20], \"...\" if len(graph_cols) > 20 else graph_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save ──────────────────────────────────────────────────────────────────────\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved: {OUTPUT_PATH} ({len(df):,} rows, {df.shape[1]} columns)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
