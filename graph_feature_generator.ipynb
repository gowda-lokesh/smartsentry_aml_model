{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Feature Generator\n",
    "---\n",
    "**Purpose:** Add graph-based AML features to the transaction table (with additional features).\n",
    "\n",
    "**Input:** `transaction_additional_feature.parquet` (or CSV fallback) from `outputs/`.\n",
    "\n",
    "**Output:** `transaction_with_graph_features.parquet` in `outputs/`.\n",
    "\n",
    "**Features:**\n",
    "- **Sender-side:** in/out degree, total inflow/outflow, unique counterparties, repeat counterparty ratio (30d).\n",
    "- **Receiver-side:** in/out degree, unique senders (30d) — for internal transfers.\n",
    "- **High-signal:** pass-through ratios (24h/7d), outflow-to-inflow ratio, avg time gap in→out, accounts/devices per device/account, device shared high-risk ratio, shared device fraud count.\n",
    "\n",
    "Config is in `config/base_config.ipynb` under `GRAPH_FEATURE_CONFIG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n",
      "  Input:  outputs/transaction_additional_feature.parquet\n",
      "  Output: outputs/transaction_with_graph_features.parquet\n",
      "  Rolling: 30d / 7d / 24h\n"
     ]
    }
   ],
   "source": [
    "# ── Imports & config ─────────────────────────────────────────────────────────\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Config: set GRAPH_FEATURE_CONFIG in config/base_config.ipynb (section 11b) or use inline below.\n",
    "try:\n",
    "    from config.base_config import GRAPH_FEATURE_CONFIG\n",
    "except Exception:\n",
    "    GRAPH_FEATURE_CONFIG = {\n",
    "        \"input_path\"       : \"transaction_additional_feature.parquet\",\n",
    "        \"output_path\"      : \"transaction_with_graph_features.parquet\",\n",
    "        \"input_dir\"        : \"outputs\",\n",
    "        \"rolling_days\"     : 30,\n",
    "        \"rolling_days_7d\"  : 7,\n",
    "        \"rolling_hours_24h\": 24,\n",
    "        \"features_enabled\" : [\n",
    "            \"sender_in_degree_30d\", \"sender_out_degree_30d\", \"sender_total_inflow_30d\",\n",
    "            \"sender_total_outflow_30d\", \"sender_unique_counterparties_30d\", \"sender_repeat_counterparty_ratio\",\n",
    "            \"receiver_in_degree_30d\", \"receiver_account_outflow_30d\", \"receiver_unique_senders_30d\",\n",
    "            # Renamed from pass_through_ratio_* — these measure volume balance, NOT transaction-level matching\n",
    "            \"inflow_outflow_volume_balance_ratio_24h\", \"inflow_outflow_volume_balance_ratio_7d\",\n",
    "            \"outflow_to_inflow_ratio_7d\",\n",
    "            \"avg_time_gap_in_out\", \"accounts_per_device\", \"devices_per_account\",\n",
    "            \"device_shared_high_risk_ratio\", \"shared_device_fraud_count\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "INPUT_DIR   = Path(GRAPH_FEATURE_CONFIG[\"input_dir\"])\n",
    "INPUT_PATH  = INPUT_DIR / GRAPH_FEATURE_CONFIG[\"input_path\"]\n",
    "OUTPUT_PATH = INPUT_DIR / GRAPH_FEATURE_CONFIG[\"output_path\"]\n",
    "ROLLING_30D = f\"{GRAPH_FEATURE_CONFIG['rolling_days']}d\"\n",
    "ROLLING_7D  = f\"{GRAPH_FEATURE_CONFIG['rolling_days_7d']}d\"\n",
    "ROLLING_24H = f\"{GRAPH_FEATURE_CONFIG['rolling_hours_24h']}h\"\n",
    "FEATURES    = GRAPH_FEATURE_CONFIG[\"features_enabled\"]\n",
    "\n",
    "def load_transactions() -> pd.DataFrame:\n",
    "    if INPUT_PATH.suffix == \".parquet\" and INPUT_PATH.exists():\n",
    "        return pd.read_parquet(INPUT_PATH)\n",
    "    csv_path = INPUT_PATH.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        return df\n",
    "    alt = INPUT_DIR / \"transactions_additional_features.csv\"\n",
    "    if alt.exists():\n",
    "        df = pd.read_csv(alt)\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        return df\n",
    "    fallback = INPUT_DIR / \"transactions.csv\"\n",
    "    if fallback.exists():\n",
    "        df = pd.read_csv(fallback)\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        return df\n",
    "    raise FileNotFoundError(f\"No input found. Tried: {INPUT_PATH}, {csv_path}, {alt}, {fallback}\")\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(f\"  Input:  {INPUT_PATH}\")\n",
    "print(f\"  Output: {OUTPUT_PATH}\")\n",
    "print(f\"  Rolling: 30d / 7d / 24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 86,992 rows, 109 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>sender_account_id</th>\n",
       "      <th>receiver_account_id</th>\n",
       "      <th>beneficiary_id</th>\n",
       "      <th>device_id</th>\n",
       "      <th>amount</th>\n",
       "      <th>channel</th>\n",
       "      <th>debit_credit</th>\n",
       "      <th>...</th>\n",
       "      <th>rule_very_high_risk_offshore</th>\n",
       "      <th>rule_low_kyc_offshore</th>\n",
       "      <th>rule_low_income_large_txn</th>\n",
       "      <th>rule_vpn_offshore</th>\n",
       "      <th>rule_emulator_crypto</th>\n",
       "      <th>rule_new_account_offshore</th>\n",
       "      <th>rule_new_acct_high_cust_velocity</th>\n",
       "      <th>rule_trigger_count</th>\n",
       "      <th>max_rule_severity</th>\n",
       "      <th>weighted_rule_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 00:04:29</td>\n",
       "      <td>T60660</td>\n",
       "      <td>C45</td>\n",
       "      <td>A1209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B244</td>\n",
       "      <td>D134</td>\n",
       "      <td>15438.75</td>\n",
       "      <td>web</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01 00:04:35</td>\n",
       "      <td>T6942</td>\n",
       "      <td>C395</td>\n",
       "      <td>A139</td>\n",
       "      <td>A1051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D597</td>\n",
       "      <td>17698.42</td>\n",
       "      <td>web</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp transaction_id customer_id sender_account_id  \\\n",
       "0 2025-09-01 00:04:29         T60660         C45             A1209   \n",
       "1 2025-09-01 00:04:35          T6942        C395              A139   \n",
       "\n",
       "  receiver_account_id beneficiary_id device_id    amount channel debit_credit  \\\n",
       "0                 NaN           B244      D134  15438.75     web        debit   \n",
       "1               A1051            NaN      D597  17698.42     web        debit   \n",
       "\n",
       "   ... rule_very_high_risk_offshore  rule_low_kyc_offshore  \\\n",
       "0  ...                            0                      0   \n",
       "1  ...                            0                      0   \n",
       "\n",
       "  rule_low_income_large_txn  rule_vpn_offshore  rule_emulator_crypto  \\\n",
       "0                         0                  0                     0   \n",
       "1                         0                  0                     0   \n",
       "\n",
       "   rule_new_account_offshore rule_new_acct_high_cust_velocity  \\\n",
       "0                          0                                0   \n",
       "1                          0                                0   \n",
       "\n",
       "  rule_trigger_count max_rule_severity weighted_rule_score  \n",
       "0                  2                 3                   5  \n",
       "1                  1                 2                   2  \n",
       "\n",
       "[2 rows x 109 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Load data ─────────────────────────────────────────────────────────────────\n",
    "df = load_transactions()\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "required = [\"transaction_id\", \"sender_account_id\", \"receiver_account_id\", \"beneficiary_id\", \"device_id\", \"timestamp\", \"amount\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "if \"label\" not in df.columns:\n",
    "    df[\"label\"] = 0\n",
    "if \"high_risk_beneficiary\" not in df.columns:\n",
    "    df[\"high_risk_beneficiary\"] = 0\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows, {df.shape[1]} columns.\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflow events (internal): 32888\n",
      "Outflow events (all): 86992\n"
     ]
    }
   ],
   "source": [
    "# ── Counterparty column (receiver account or beneficiary) ─────────────────────\n",
    "df[\"_counterparty\"] = df[\"receiver_account_id\"].fillna(df[\"beneficiary_id\"].astype(str))\n",
    "\n",
    "# Inflow events: rows where this account is the receiver (internal only)\n",
    "inflow = df.loc[df[\"receiver_account_id\"].notna(), [\"receiver_account_id\", \"sender_account_id\", \"timestamp\", \"amount\"]].copy()\n",
    "inflow = inflow.rename(columns={\"receiver_account_id\": \"account_id\", \"sender_account_id\": \"sender_id\"})\n",
    "inflow = inflow.sort_values([\"account_id\", \"timestamp\"])\n",
    "\n",
    "# Outflow events: all rows (sender, counterparty, device, amount, timestamp)\n",
    "outflow = df[[\"sender_account_id\", \"timestamp\", \"amount\", \"_counterparty\", \"device_id\", \"label\", \"high_risk_beneficiary\"]].copy()\n",
    "outflow = outflow.rename(columns={\"sender_account_id\": \"account_id\"})\n",
    "outflow = outflow.sort_values([\"account_id\", \"timestamp\"])\n",
    "\n",
    "print(\"Inflow events (internal):\", len(inflow))\n",
    "print(\"Outflow events (all):\", len(outflow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender outflow rolling: (86992, 7)\n",
      "Sender inflow rolling: (32888, 4)\n"
     ]
    }
   ],
   "source": [
    "# ── Sender-side rolling 30d (outflow + inflow for sender) ──────────────────────\n",
    "# Rolling nunique not supported on object cols; compute per-group with time window.\n",
    "def _rolling_nunique(df, group_col, time_col, value_col, window_td, out_name):\n",
    "    out = []\n",
    "    for _, g in df.groupby(group_col, sort=False):\n",
    "        g = g.sort_values(time_col).reset_index(drop=True)\n",
    "        ts = pd.to_datetime(g[time_col]).values\n",
    "        vals = g[value_col].values\n",
    "        for i in range(len(g)):\n",
    "            t_end = ts[i]\n",
    "            t_start = t_end - np.timedelta64(int(window_td.total_seconds() * 1e9), \"ns\")\n",
    "            # side=\"right\" → first index where ts[j] > t_start, so transactions at exactly\n",
    "            # t_start ARE included (window is [t_start, t_end] inclusive on both ends).\n",
    "            # Example: for a 30d window, a transaction exactly 30 days ago at the same\n",
    "            # second is included. This is intentional for rolling aggregation consistency.\n",
    "            start_idx = np.searchsorted(ts, t_start, side=\"right\")\n",
    "            window_vals = vals[start_idx : i + 1]\n",
    "            out.append((g[group_col].iloc[i], pd.Timestamp(t_end), len(np.unique(window_vals))))\n",
    "    res = pd.DataFrame(out, columns=[group_col, time_col, out_name])\n",
    "    res[time_col] = pd.to_datetime(res[time_col])\n",
    "    return res\n",
    "\n",
    "WINDOW_30D_TD = pd.Timedelta(ROLLING_30D)\n",
    "\n",
    "def rolling_sender_out():\n",
    "    o = outflow.set_index(\"timestamp\", drop=False)\n",
    "    g = o.groupby(\"account_id\", group_keys=False)\n",
    "    r = g.rolling(ROLLING_30D, on=\"timestamp\").agg({\"amount\": [\"count\", \"sum\"]}).reset_index()\n",
    "    r.columns = [\"account_id\", \"timestamp\", \"sender_out_degree_30d\", \"sender_total_outflow_30d\"]\n",
    "    cp = _rolling_nunique(outflow, \"account_id\", \"timestamp\", \"_counterparty\", WINDOW_30D_TD, \"sender_unique_counterparties_30d\")\n",
    "    dev = _rolling_nunique(outflow, \"account_id\", \"timestamp\", \"device_id\", WINDOW_30D_TD, \"devices_per_account\")\n",
    "    r = r.merge(cp, on=[\"account_id\", \"timestamp\"]).merge(dev, on=[\"account_id\", \"timestamp\"])\n",
    "    r[\"sender_repeat_counterparty_ratio\"] = (\n",
    "        1 - r[\"sender_unique_counterparties_30d\"] / r[\"sender_out_degree_30d\"].replace(0, np.nan)\n",
    "    ).fillna(0).round(4)\n",
    "    return r\n",
    "\n",
    "def rolling_sender_in():\n",
    "    i = inflow.set_index(\"timestamp\", drop=False)\n",
    "    r = i.groupby(\"account_id\", group_keys=False).rolling(ROLLING_30D, on=\"timestamp\").agg(\n",
    "        {\"amount\": [\"count\", \"sum\"]}\n",
    "    ).reset_index()\n",
    "    r.columns = [\"account_id\", \"timestamp\", \"sender_in_degree_30d\", \"sender_total_inflow_30d\"]\n",
    "    return r\n",
    "\n",
    "sender_out_30 = rolling_sender_out()\n",
    "sender_in_30  = rolling_sender_in()\n",
    "print(\"Sender outflow rolling:\", sender_out_30.shape)\n",
    "print(\"Sender inflow rolling:\", sender_in_30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Receiver inflow rolling: (32888, 5)\n"
     ]
    }
   ],
   "source": [
    "# ── Receiver-side rolling 30d (for internal transfers: receiver = account) ─────\n",
    "def rolling_receiver():\n",
    "    i = inflow.copy()\n",
    "    i = i.rename(columns={\"account_id\": \"receiver_id\", \"sender_id\": \"sender_id\"})\n",
    "    i = i.set_index(\"timestamp\", drop=False)\n",
    "    r = i.groupby(\"receiver_id\", group_keys=False).rolling(ROLLING_30D, on=\"timestamp\").agg({\"amount\": [\"count\", \"sum\"]}).reset_index()\n",
    "    r.columns = [\"account_id\", \"timestamp\", \"receiver_in_degree_30d\", \"receiver_total_inflow_30d\"]\n",
    "    su = _rolling_nunique(i.reset_index(drop=True), \"receiver_id\", \"timestamp\", \"sender_id\", WINDOW_30D_TD, \"receiver_unique_senders_30d\")\n",
    "    su = su.rename(columns={\"receiver_id\": \"account_id\"})\n",
    "    r = r.merge(su, on=[\"account_id\", \"timestamp\"])\n",
    "    return r\n",
    "\n",
    "# receiver_out_degree_30d merged later from sender_out_30 (same account as sender)\n",
    "receiver_in_30 = rolling_receiver()\n",
    "print(\"Receiver inflow rolling:\", receiver_in_30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24h/7d inflow and outflow rolling done.\n"
     ]
    }
   ],
   "source": [
    "# ── 24h / 7d inflow & outflow for pass-through and ratio ─────────────────────────\n",
    "def rolling_inflow_24h_7d():\n",
    "    i = inflow.set_index(\"timestamp\", drop=False)\n",
    "    g = i.groupby(\"account_id\", group_keys=False)\n",
    "    r24 = g.rolling(ROLLING_24H, on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r24 = r24.rename(columns={\"amount\": \"sender_total_inflow_24h\"})\n",
    "    r7  = g.rolling(ROLLING_7D,  on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r7  = r7.rename(columns={\"amount\": \"sender_total_inflow_7d\"})\n",
    "    return r24, r7\n",
    "\n",
    "def rolling_outflow_24h_7d():\n",
    "    o = outflow.set_index(\"timestamp\", drop=False)\n",
    "    g = o.groupby(\"account_id\", group_keys=False)\n",
    "    r24 = g.rolling(ROLLING_24H, on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r24 = r24.rename(columns={\"amount\": \"sender_total_outflow_24h\"})\n",
    "    r7  = g.rolling(ROLLING_7D,  on=\"timestamp\").agg({\"amount\": \"sum\"}).reset_index()\n",
    "    r7  = r7.rename(columns={\"amount\": \"sender_total_outflow_7d\"})\n",
    "    return r24, r7\n",
    "\n",
    "in_24, in_7  = rolling_inflow_24h_7d()\n",
    "out_24, out_7 = rolling_outflow_24h_7d()\n",
    "print(\"24h/7d inflow and outflow rolling done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device rolling: (86994, 5)\n"
     ]
    }
   ],
   "source": [
    "# ── Device rolling: accounts_per_device, device_shared_high_risk_ratio, shared_device_fraud_count ──\n",
    "dev = df[[\"device_id\", \"timestamp\", \"sender_account_id\", \"label\", \"high_risk_beneficiary\"]].copy()\n",
    "\n",
    "# Fix Issue 4: union flag avoids double-counting transactions that are BOTH fraud and high-risk.\n",
    "# A single transaction with label=1 AND high_risk_beneficiary=1 is counted only once.\n",
    "dev[\"_fraud_or_high_risk\"] = ((dev[\"label\"] == 1) | (dev[\"high_risk_beneficiary\"] == 1)).astype(int)\n",
    "\n",
    "dev = dev.sort_values([\"device_id\", \"timestamp\"]).set_index(\"timestamp\", drop=False)\n",
    "g = dev.groupby(\"device_id\", group_keys=False)\n",
    "r = g.rolling(ROLLING_30D, on=\"timestamp\").agg({\n",
    "    \"sender_account_id\": \"count\",\n",
    "    \"label\": \"sum\",\n",
    "    \"_fraud_or_high_risk\": \"sum\",   # union count: no double-counting\n",
    "}).reset_index()\n",
    "r.columns = [\"device_id\", \"timestamp\", \"_dev_txn_count\", \"_dev_fraud_count\", \"_dev_high_risk_unique_count\"]\n",
    "\n",
    "dev_df = dev.reset_index(drop=True)\n",
    "nacc = _rolling_nunique(dev_df, \"device_id\", \"timestamp\", \"sender_account_id\", WINDOW_30D_TD, \"accounts_per_device\")\n",
    "device_rolling = r.merge(nacc, on=[\"device_id\", \"timestamp\"])\n",
    "device_rolling = device_rolling[[\"device_id\", \"timestamp\", \"accounts_per_device\",\n",
    "                                  \"_dev_txn_count\", \"_dev_fraud_count\", \"_dev_high_risk_unique_count\"]]\n",
    "\n",
    "# Use union count (fraud OR high_risk) to avoid inflating the ratio via double-counting\n",
    "device_rolling[\"device_shared_high_risk_ratio\"] = (\n",
    "    device_rolling[\"_dev_high_risk_unique_count\"]\n",
    "    / device_rolling[\"_dev_txn_count\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "device_rolling[\"shared_device_fraud_count\"] = device_rolling[\"_dev_fraud_count\"]\n",
    "device_rolling = device_rolling.drop(columns=[\"_dev_txn_count\", \"_dev_fraud_count\", \"_dev_high_risk_unique_count\"])\n",
    "print(\"Device rolling:\", device_rolling.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling avg time gap (in→out) computed: 32,608 rows (one per inflow event).\n"
     ]
    }
   ],
   "source": [
    "# ── avg_time_gap_in_out: rolling 24h average time from inflow to next outflow ──\n",
    "# Fix Issue 1: previously this computed a single global average per account (all inflows averaged\n",
    "# into one value), which was then broadcast to every transaction for that account.\n",
    "# Now we compute a rolling 24h average: for each inflow event, we find the gap to the\n",
    "# next outflow, then roll those gaps over a 24h window — producing one value per\n",
    "# (account_id, inflow_timestamp). The merge step uses merge_asof to assign the most recent\n",
    "# rolling avg to each sender transaction (temporal alignment instead of global broadcast).\n",
    "\n",
    "def rolling_avg_time_gap(inflow_df: pd.DataFrame, outflow_df: pd.DataFrame, window_hours: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Per-account rolling average of the time gap from an inflow event to the next outflow.\n",
    "\n",
    "    Returns a DataFrame with columns: [account_id, timestamp, avg_time_gap_in_out]\n",
    "    where timestamp = inflow event timestamp and avg_time_gap_in_out = rolling mean\n",
    "    of gap_sec values within the past `window_hours` hours.\n",
    "    \"\"\"\n",
    "    gaps = []\n",
    "    for acc, grp_in in inflow_df.groupby(\"account_id\"):\n",
    "        grp_out = outflow_df[outflow_df[\"account_id\"] == acc].sort_values(\"timestamp\")\n",
    "        if grp_out.empty:\n",
    "            continue\n",
    "        ts_out = pd.to_datetime(grp_out[\"timestamp\"]).values\n",
    "        for _, row in grp_in.sort_values(\"timestamp\").iterrows():\n",
    "            t_in = np.datetime64(pd.Timestamp(row[\"timestamp\"]))\n",
    "            idx = np.searchsorted(ts_out, t_in, side=\"right\")\n",
    "            if idx < len(ts_out):\n",
    "                gap_sec = (ts_out[idx] - t_in) / np.timedelta64(1, \"s\")\n",
    "                gaps.append({\"account_id\": acc, \"timestamp\": pd.Timestamp(t_in), \"gap_sec\": gap_sec})\n",
    "\n",
    "    if not gaps:\n",
    "        return pd.DataFrame(columns=[\"account_id\", \"timestamp\", \"avg_time_gap_in_out\"])\n",
    "\n",
    "    gap_df = pd.DataFrame(gaps).sort_values([\"account_id\", \"timestamp\"])\n",
    "    gap_df[\"timestamp\"] = pd.to_datetime(gap_df[\"timestamp\"])\n",
    "\n",
    "    # Rolling mean of gap_sec over past window_hours for each account\n",
    "    gap_df = gap_df.set_index(\"timestamp\")\n",
    "    gap_df[\"avg_time_gap_in_out\"] = (\n",
    "        gap_df.groupby(\"account_id\")[\"gap_sec\"]\n",
    "        .rolling(f\"{window_hours}h\", min_periods=1)\n",
    "        .mean()\n",
    "        .droplevel(0)           # drop the extra account_id level from MultiIndex\n",
    "    )\n",
    "    return gap_df.reset_index()[[\"account_id\", \"timestamp\", \"avg_time_gap_in_out\"]]\n",
    "\n",
    "avg_gap_df = rolling_avg_time_gap(inflow, outflow, window_hours=24)\n",
    "print(f\"Rolling avg time gap (in→out) computed: {len(avg_gap_df):,} rows (one per inflow event).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged. Columns: 134\n"
     ]
    }
   ],
   "source": [
    "# ── Merge all rolling stats back to main df (by exact timestamp + key) ──────────\n",
    "# Rolling tables have one row per (key, timestamp) so we merge on (key, timestamp).\n",
    "\n",
    "merge_cols = [\"account_id\", \"timestamp\"]\n",
    "\n",
    "sender_out_cols = [\"sender_out_degree_30d\", \"sender_total_outflow_30d\", \"sender_unique_counterparties_30d\",\n",
    "                   \"sender_repeat_counterparty_ratio\", \"devices_per_account\"]\n",
    "df = df.merge(\n",
    "    sender_out_30[merge_cols + sender_out_cols],\n",
    "    left_on=[\"sender_account_id\", \"timestamp\"],\n",
    "    right_on=merge_cols,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_sout\")\n",
    ").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "sender_in_cols = [\"sender_in_degree_30d\", \"sender_total_inflow_30d\"]\n",
    "df = df.merge(\n",
    "    sender_in_30[merge_cols + sender_in_cols],\n",
    "    left_on=[\"sender_account_id\", \"timestamp\"],\n",
    "    right_on=merge_cols,\n",
    "    how=\"left\"\n",
    ")\n",
    "if \"account_id\" in df.columns and df[\"account_id\"].equals(df[\"sender_account_id\"]):\n",
    "    df = df.drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "in_24_cols  = [\"sender_total_inflow_24h\"]\n",
    "in_7_cols   = [\"sender_total_inflow_7d\"]\n",
    "out_24_cols = [\"sender_total_outflow_24h\"]\n",
    "out_7_cols  = [\"sender_total_outflow_7d\"]\n",
    "df = df.merge(in_24[merge_cols + in_24_cols],   left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "df = df.merge(in_7[merge_cols + in_7_cols],     left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "df = df.merge(out_24[merge_cols + out_24_cols], left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "df = df.merge(out_7[merge_cols + out_7_cols],   left_on=[\"sender_account_id\", \"timestamp\"], right_on=merge_cols, how=\"left\").drop(columns=[\"account_id\"], errors=\"ignore\")\n",
    "\n",
    "# Fix Issue 3: renamed from pass_through_ratio_* to clarify that this is a VOLUME BALANCE\n",
    "# ratio (total inflow matched by total outflow) — NOT transaction-level amount matching.\n",
    "# Limitation: the same money is not guaranteed to flow through; the account may use its own\n",
    "# pre-existing balance for outflows. For true pass-through detection, transaction-level\n",
    "# matching (inflow → outflow within X hours at ±5% amount tolerance) would be needed.\n",
    "df[\"inflow_outflow_volume_balance_ratio_24h\"] = (\n",
    "    np.minimum(df[\"sender_total_inflow_24h\"].fillna(0), df[\"sender_total_outflow_24h\"].fillna(0))\n",
    "    / df[\"sender_total_inflow_24h\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "df[\"inflow_outflow_volume_balance_ratio_7d\"] = (\n",
    "    np.minimum(df[\"sender_total_inflow_7d\"].fillna(0), df[\"sender_total_outflow_7d\"].fillna(0))\n",
    "    / df[\"sender_total_inflow_7d\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "df[\"outflow_to_inflow_ratio_7d\"] = (\n",
    "    df[\"sender_total_outflow_7d\"].fillna(0) / df[\"sender_total_inflow_7d\"].replace(0, np.nan)\n",
    ").fillna(0).round(4)\n",
    "\n",
    "df = df.merge(\n",
    "    device_rolling[[\"device_id\", \"timestamp\", \"accounts_per_device\", \"device_shared_high_risk_ratio\", \"shared_device_fraud_count\"]],\n",
    "    on=[\"device_id\", \"timestamp\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fix Issue 1: manual as-of join using searchsorted — avoids merge_asof's strict global\n",
    "# sort requirement (which is tricky when `by` groups have interleaved timestamps).\n",
    "# For each sender account, the rolling avg gap table is sorted by (account, timestamp),\n",
    "# and we use searchsorted to find the latest entry at or before each transaction timestamp.\n",
    "avg_gap_lookup = (\n",
    "    avg_gap_df\n",
    "    .rename(columns={\"account_id\": \"sender_account_id\"})\n",
    "    .sort_values([\"sender_account_id\", \"timestamp\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_sender_ids = df[\"sender_account_id\"].values\n",
    "df_timestamps = df[\"timestamp\"].values\n",
    "avg_gap_vals  = np.full(len(df), np.nan)\n",
    "\n",
    "for acc, grp in avg_gap_lookup.groupby(\"sender_account_id\", sort=False):\n",
    "    acc_mask = df_sender_ids == acc\n",
    "    if not acc_mask.any():\n",
    "        continue\n",
    "    grp_ts   = grp[\"timestamp\"].values                # sorted within account\n",
    "    grp_vals = grp[\"avg_time_gap_in_out\"].values\n",
    "    acc_ts   = df_timestamps[acc_mask]\n",
    "    # searchsorted finds insertion point; -1 gives the latest entry <= acc_ts\n",
    "    idxs  = np.searchsorted(grp_ts, acc_ts, side=\"right\") - 1\n",
    "    valid = idxs >= 0\n",
    "    result = np.full(len(acc_ts), np.nan)\n",
    "    result[valid] = grp_vals[idxs[valid]]\n",
    "    avg_gap_vals[acc_mask] = result\n",
    "\n",
    "df[\"avg_time_gap_in_out\"] = avg_gap_vals\n",
    "\n",
    "receiver_in_cols = [\"receiver_in_degree_30d\", \"receiver_total_inflow_30d\", \"receiver_unique_senders_30d\"]\n",
    "receiver_in_30_renamed = receiver_in_30.rename(columns={\"account_id\": \"receiver_account_id\"})\n",
    "df = df.merge(\n",
    "    receiver_in_30_renamed[[\"receiver_account_id\", \"timestamp\"] + receiver_in_cols],\n",
    "    on=[\"receiver_account_id\", \"timestamp\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fix Issue 2: renamed receiver_out_degree_30d → receiver_account_outflow_30d.\n",
    "# The original name implied \"outgoing transactions from an account *as a receiver*\", which\n",
    "# is contradictory. The correct reading is: outflow activity of the account that *acts as*\n",
    "# receiver in this transaction (i.e., how active is the receiver as a sender in 30d).\n",
    "receiver_out_30_for_merge = sender_out_30[[\"account_id\", \"timestamp\", \"sender_out_degree_30d\"]].rename(\n",
    "    columns={\"account_id\": \"receiver_account_id\", \"sender_out_degree_30d\": \"receiver_account_outflow_30d\"}\n",
    ")\n",
    "df = df.merge(receiver_out_30_for_merge, on=[\"receiver_account_id\", \"timestamp\"], how=\"left\")\n",
    "\n",
    "# Fix Issue 5: external transfers (receiver_account_id IS NULL, ~62% of rows) get NaN for\n",
    "# all receiver-side features after the left joins above. Fill with 0 to explicitly signal\n",
    "# \"not an internal transfer\" rather than treating missing as unknown or imputing the median.\n",
    "receiver_feature_cols = receiver_in_cols + [\"receiver_account_outflow_30d\"]\n",
    "for col in receiver_feature_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "print(\"Merged. Columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns: 129\n",
      "Graph feature columns: ['sender_out_degree_30d', 'sender_total_outflow_30d', 'sender_unique_counterparties_30d', 'sender_repeat_counterparty_ratio', 'devices_per_account', 'sender_in_degree_30d', 'sender_total_inflow_30d', 'inflow_outflow_volume_balance_ratio_24h', 'inflow_outflow_volume_balance_ratio_7d', 'outflow_to_inflow_ratio_7d', 'accounts_per_device', 'device_shared_high_risk_ratio', 'shared_device_fraud_count', 'avg_time_gap_in_out', 'receiver_in_degree_30d', 'receiver_total_inflow_30d', 'receiver_unique_senders_30d', 'receiver_account_outflow_30d'] ['sender_out_degree_30d', 'sender_total_outflow_30d', 'sender_unique_counterparties_30d', 'sender_repeat_counterparty_ratio', 'devices_per_account', 'sender_in_degree_30d', 'sender_total_inflow_30d', 'inflow_outflow_volume_balance_ratio_24h', 'inflow_outflow_volume_balance_ratio_7d', 'outflow_to_inflow_ratio_7d', 'accounts_per_device', 'device_shared_high_risk_ratio', 'shared_device_fraud_count', 'avg_time_gap_in_out', 'receiver_in_degree_30d', 'receiver_total_inflow_30d', 'receiver_unique_senders_30d', 'receiver_account_outflow_30d']\n"
     ]
    }
   ],
   "source": [
    "# ── Drop helper columns and keep only requested features if config says so ─────\n",
    "df = df.drop(columns=[\"_counterparty\"], errors=\"ignore\")\n",
    "\n",
    "optional_drop = [\"sender_total_inflow_24h\", \"sender_total_outflow_24h\", \"sender_total_inflow_7d\", \"sender_total_outflow_7d\"]\n",
    "for c in optional_drop:\n",
    "    if c in df.columns and c not in FEATURES:\n",
    "        df = df.drop(columns=[c], errors=\"ignore\")\n",
    "\n",
    "print(\"Final columns:\", len(df.columns))\n",
    "graph_cols = [c for c in df.columns if any(x in c for x in [\"degree\", \"inflow\", \"outflow\", \"counterpart\", \"pass_through\", \"ratio_7d\", \"avg_time_gap\", \"accounts_per_device\", \"devices_per\", \"device_shared\", \"shared_device_fraud\", \"receiver_in\", \"receiver_out\", \"receiver_unique\"])]\n",
    "print(\"Graph feature columns:\", graph_cols[:20], \"...\" if len(graph_cols) > 20 else graph_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: outputs/transaction_with_graph_features.parquet (86,998 rows, 129 columns)\n"
     ]
    }
   ],
   "source": [
    "# ── Save ──────────────────────────────────────────────────────────────────────\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved: {OUTPUT_PATH} ({len(df):,} rows, {df.shape[1]} columns)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
